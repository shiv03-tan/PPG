# -*- coding: utf-8 -*-
"""correlation (4) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14gFgWHW8rfOm5H32IdD6XDEFw-hJYUSj

**loading libraries**
"""

import pandas as pd
import scipy.io
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

"""**Loading dataset**"""

mat_file = scipy.io.loadmat(r'/content/Biosec2_raw_data (2).mat')
signal1 = mat_file['Data_cell']

s1=signal1[1,0,0]
plt.figure(1)
plt.xlim(10,1000)
plt.plot(s1,'-g')
# plt.plot(s3,'-r')
plt.title('Normalized Raw PPG Signal (removed DC offset)')
plt.show()

for i in range(100):
    s = signal1[i, 0, 0]
    slen = len(s)
    s = np.reshape(s, slen)
    s = s - np.mean(s)
    signal1[i, 0, 0] = s
    
    s = s / np.max(np.abs(s))

print(    signal1[99, 0, 0])
#plt.plot(signal1[99, 0, 0])

plt.figure(1)
plt.xlim(10,600)
plt.plot(s,'-g')
# plt.plot(s3,'-r')
plt.title('Normalized Raw PPG Signal (removed DC offset)')
plt.show()

# Remove both low and high frequency noise
low_cutoff_freq = 0.5 # Cutoff frequency for low-pass filter in Hz
high_cutoff_freq = 10 # Cutoff frequency for high-pass filter in Hz
sampling_rate = 100 # Sampling rate is the number of samples per second
nyquist_freq = 0.5 * sampling_rate
low_normal_cutoff = low_cutoff_freq / nyquist_freq
high_normal_cutoff = high_cutoff_freq / nyquist_freq

# Apply Butterworth filter to remove low-frequency noise
b_low, a_low = butter(4, low_normal_cutoff, btype='low')
filtered_signal1 = lfilter(b_low, a_low, s)

# Apply Butterworth filter to remove high-frequency noise
b_high, a_high = butter(4, high_normal_cutoff, btype='high')
filtered_signal2 = lfilter(b_high, a_high, filtered_signal1)

import matplotlib.pyplot as plt

# Plot the original signal
plt.plot(s, label='Original Signal')

# Plot the filtered signal
plt.plot(filtered_signal2, label='Filtered Signal')
plt.plot(filtered_signal1, label='Filtered Signal')

# Set the plot title and axis labels
plt.title('Signal with Low and High Frequency Noise Removed')
plt.xlim(10,1000)
plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')

# Show the legend
plt.legend()

# Show the plot
plt.show()

"""**removing noise using buterworth bandpass filter**"""

# Apply a Butterworth bandpass filter
low_cutoff_freq = 0.5 # Cutoff frequency for low-pass filter in Hz
high_cutoff_freq = 10 # Cutoff frequency for high-pass filter in Hz
sampling_rate = 100 # Sampling rate is the number of samples per second
nyquist_freq = 0.5 * sampling_rate
low_normal_cutoff = low_cutoff_freq / nyquist_freq
high_normal_cutoff = high_cutoff_freq / nyquist_freq

# Generate filter coefficients
b, a = butter(4, [low_normal_cutoff, high_normal_cutoff], btype='band')

# Apply the filter to the signal
filtered_signal = lfilter(b, a, s)

# Plot the pre-processed signal
plt.figure(2)
plt.xlim(1,1000)
plt.plot(filtered_signal)
plt.title('Pre-processed PPG Signal')
plt.show()

import matplotlib.pyplot as plt

# Load the data
mat_file = scipy.io.loadmat('/content/Biosec2_raw_data (2).mat')
signal1 = mat_file['Data_cell']

# Extract the first signal and preprocess it
s = signal1[1, 0, 0]
s = np.reshape(s, 9240)
s = s - np.mean(s)

# Plot the original signal
plt.figure(figsize=(10, 6))
plt.plot(s)
plt.title('Original PPG Signal')

# Apply a Butterworth bandpass filter
low_cutoff_freq = 0.5 # Cutoff frequency for low-pass filter in Hz
high_cutoff_freq = 10 # Cutoff frequency for high-pass filter in Hz
sampling_rate = 100 # Sampling rate is the number of samples per second
nyquist_freq = 0.5 * sampling_rate
low_normal_cutoff = low_cutoff_freq / nyquist_freq
high_normal_cutoff = high_cutoff_freq / nyquist_freq

# Generate filter coefficients
b, a = butter(4, [low_normal_cutoff, high_normal_cutoff], btype='band')

# Apply the filter to the signal
filtered_signal = lfilter(b, a, s)

# Plot the pre-processed signal
plt.figure(figsize=(10, 6))
plt.plot(filtered_signal)
plt.title('Filtered PPG Signal')

# Show the plots
plt.show()

"""**Compute analysis indicators**"""

# Compute analysis indicators
avg = np.mean(filtered_signal)
std = np.std(filtered_signal)
var = np.var(filtered_signal)
entropy = -np.sum(np.square(filtered_signal) * np.log(np.square(filtered_signal)))
mean = np.mean(np.abs(filtered_signal))
peak_to_peak = np.max(filtered_signal) - np.min(filtered_signal)

# Print analysis indicators
print('Average:', avg)
print('Standard deviation:', std)
print('Variance:', var)
print('Entropy:', entropy)
print('Mean:', mean)
print('Peak-to-peak:', peak_to_peak)

import matplotlib.pyplot as plt

# Compute analysis indicators
avg = np.mean(filtered_signal)
std = np.std(filtered_signal)
var = np.var(filtered_signal)
entropy = -np.sum(np.square(filtered_signal) * np.log(np.square(filtered_signal)))
mean = np.mean(np.abs(filtered_signal))
peak_to_peak = np.max(filtered_signal) - np.min(filtered_signal)

# Create a bar plot
labels = ['Average', 'Standard deviation', 'Variance', 'Entropy', 'Mean', 'Peak-to-peak']
values = [avg, std, var, mean, entropy, peak_to_peak]
plt.bar(labels, values)

# Add labels and a title to the plot
plt.xlabel('Analysis Indicators')
plt.ylabel('Value')
plt.title('Signal Analysis Indicators')

# Show the plot
plt.show()

import matplotlib.pyplot as plt

# Compute analysis indicators
avg = np.mean(filtered_signal)
std = np.std(filtered_signal)
var = np.var(filtered_signal)
entropy = -np.sum(np.square(filtered_signal) * np.log(np.square(filtered_signal)))
mean = np.mean(np.abs(filtered_signal))
peak_to_peak = np.max(filtered_signal) - np.min(filtered_signal)

# Plot each analysis indicator separately
plt.figure(figsize=(10, 6))

plt.subplot(2, 3, 1)
plt.plot(filtered_signal)
plt.title('Filtered Signal')

plt.subplot(2, 3, 2)
plt.bar('Average', avg)
plt.title('Average')

plt.subplot(2, 3, 3)
plt.bar('Standard deviation', std)
plt.title('Standard deviation')

plt.subplot(2, 3, 4)
plt.bar('Variance', var)
plt.title('Variance')

plt.subplot(2, 3, 5)
plt.bar('Entropy', entropy)
plt.title('Entropy')

plt.subplot(2, 3, 6)
plt.bar('Mean', mean)
plt.bar('Peak-to-peak', peak_to_peak)
plt.title('Mean and Peak-to-peak')

# Show the plots
plt.tight_layout()
plt.show()

#Apply a low-pass filter to remove high-frequency noise from the signal.
# Remove high-frequency noise
cutoff_freq = 0.5 # Cutoff frequency in Hz
sampling_rate = 100 # Sampling rate is the number of samples per second
nyquist_freq = 0.5 * sampling_rate
normal_cutoff = cutoff_freq / nyquist_freq
b, a = butter(1, 3, btype='high', analog=True)
filtered_signal1 = lfilter(b, a, s )

# Plot the pre-processed signal
plt.figure(2)
plt.xlim(1,800)
plt.plot(filtered_signal1)
plt.title('Pre-processed PPG Signal')
plt.show()

"""**Segmentation**"""

segment_length = 100 # Length of each segment
segmented_signal = [] # List to hold the segmented signal

for i in range(0, len(filtered_signal), segment_length):
    # Find peaks in the current segment
    segment_peaks, _ = find_peaks(filtered_signal[i:i+segment_length])
    
    # Add the segment to the segmented signal list
    segmented_signal.append(filtered_signal[i:i+segment_length][segment_peaks])

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Plot the original signal in blue
plt.plot(filtered_signal, '-b')

# Plot each segment in red
for segment in segmented_signal:
    plt.plot(segment, '-r')

plt.xlabel('Time (samples)')
plt.ylabel('Amplitude')
plt.title('Segmented Signal')
plt.show()

print(filtered_signal1)

"""**Find peaks and dips**"""

# Find peaks and dips
peaks, _ = find_peaks(filtered_signal, distance= 20)
peaks = peaks[3:] # Only include peaks from the 4th peak onwards
s=filtered_signal1[peaks[0]:]

# Plot the signal and the detected peaks and dips
fig, ax = plt.subplots(figsize=(20,10))
plt.xlim(1,1000)
ax.plot(filtered_signal , 'g', label='Filtered PPG signal')
ax.plot(peaks, filtered_signal[peaks], 'r*', label='Peaks')
ax.set_ylabel('Amplitude')
plt.title('Feature Extraction PPG Signal')
ax.legend()
plt.show()

print(filtered_signal[peaks])

dips, _ = find_peaks(-filtered_signal, distance= 25)
#peaks, _ = find_peaks(filtered_signal1, distance= 20)
systolic_peaks, _ = find_peaks(filtered_signal, distance=20 )
diastolic_peaks, _ = find_peaks(-filtered_signal, distance=15)
systolic_foots, _ = find_peaks(-filtered_signal, distance= 10)
diastolic_foots, _ = find_peaks(filtered_signal, distance= 15)
s=filtered_signal[dips[4]:]

print(dips)

# Plot the signal and the detected peaks and dips
fig, ax = plt.subplots(figsize=(20,10))
plt.xlim(0,1000)
#ax.plot(s, 'b', label='Raw PPG signal')
ax.plot(filtered_signal , 'g', label='Filtered PPG signal')
#ax.plot(peaks, filtered_signal[peaks], 'r*', label='Peaks')
ax.plot(dips, filtered_signal[dips], 'ko', label='Dips')
ax.plot(systolic_peaks, filtered_signal[systolic_peaks], 'yo', label='Systolic peaks')
ax.plot(diastolic_peaks, filtered_signal[diastolic_peaks], 'mo', label='Diastolic peaks')
ax.plot(systolic_foots, filtered_signal[systolic_foots], 'o', label='systolic_foots')
ax.plot(diastolic_foots, filtered_signal[diastolic_foots], 'p', label='diastolic_foots')
ax.set_xlabel('Sample number')
ax.set_ylabel('Amplitude')
plt.title('Feature Extraction PPG Signal')
ax.legend()
plt.show()

a = signal1[i:-1]
v = signal1[1:-1]
plt.plot(a)





#a = signal1[1,0,0]
#v = signal1[2,0,0]
def NormaliseCrossCorrelation(a,v):
    M = len(a)
    N = len(v)

    if type(a) is list: 
        a = np.array(a)
    if type(v) is list: 
        v = np.array(v)
    
    v_mu = np.mean(v)

    ncc_series = []
    for i in range(M-N+1):
        _a = a[i:i+N]
        a_sum = _a - np.mean(a)
        v_sum = v - v_mu
        #if(a_sum != 0 and v_sum != 0 and N != 0):
        nr = 1/N * np.sum(a_sum*v_sum) / np.sqrt(np.var(a)*np.var(v))
        ncc_series.append(nr)
  
    return sum(ncc_series)/len(ncc_series)

# Example usage
#a = np.random.rand(100)
#v = np.random.rand(10)

ncc_series = NormaliseCrossCorrelation(a, v)
print(ncc_series)
plt.plot(ncc_series, '*r')
plt.xlabel('Lag')
plt.ylabel('Normalized Cross-Correlation')
plt.title('Normalized Cross-Correlation of a and v')
plt.show()

import numpy as np

# Define the signals
num_signals = 100
signal_length = 1000
signals = np.random.randn(num_signals, signal_length)

# Define the filter
filter_length = 100
filter_signal = np.random.randn(filter_length)

# Preprocess the filter
filter_signal -= np.mean(filter_signal)
filter_signal /= np.linalg.norm(filter_signal)

# Preprocess each signal
preprocessed_signals = []
for signal in signals:
    signal -= np.mean(signal)
    signal /= np.linalg.norm(signal)
    preprocessed_signals.append(signal)

# Compute the same normalized cross-correlation for filtered_signal[1] and all other signals
result = []
for i in range(num_signals):
    if i != 1:  # skip the second signal, which is the filtered signal
        x = preprocessed_signals[i]
        corr = np.correlate(x, filter_signal, mode='valid')
        norm = np.linalg.norm(x) * np.linalg.norm(filter_signal)
        result.append(corr / norm)

# Print the result
print(result)

def NormaliseCrossCorrelation(a, v):
    M = len(a)
    if isinstance(v, np.ndarray):
        N = v.shape[0]
    elif isinstance(v, np.float64):
        N = 1
    else:
        raise TypeError('v must be a numpy array or scalar')
        
    if isinstance(a, list):
        a = np.array(a)

import numpy as np
#a = signal1[1,0,0]
#v = signal1[2,0,0]
print(a)
print(v)
ncc_series = NormaliseCrossCorrelation(a, v)
print(ncc_series)
# Initialize correlation matrix
#corr_matrix = np.zeros((n_signals, n_signals))
"""
# Compute cross-correlation for each pair of signals
for i in range(10):
  v = signal1[i]
  ncc_series = NormaliseCrossCorrelation(a, v)
  print(ncc_series)
"""

import numpy as np

def normalisecrosscorrelation(subjects, template):
    """
    Perform normalized cross-correlation between a template and multiple subjects.

    Args:
    subjects: a list of numpy arrays, where each array represents a subject's data.
    template: a numpy array representing the template to cross-correlate with.

    Returns:
    A list of normalized cross-correlation values for each subject.
    """
    n_subjects = len(subjects)
    n_template = len(template)

    corr_values = []
    for i in range(n_subjects):
        # Calculate cross-correlation between the template and the subject's data
        corr = np.correlate(subjects[i], template, mode='same')
        
        # Normalize the cross-correlation value
        norm_corr = corr / (np.linalg.norm(subjects[i]) * np.linalg.norm(template))

        # Save the normalized cross-correlation value
        corr_values.append(norm_corr)

    return corr_values

a=filtered_signal1[0:-1]
print(a)

# Generate some example data
n_subjects = 3
subjects = [signal1[i] for i in range(1, 100)]
template = signal1[0]


# Convert the input arrays to a homogeneous data type
dtype = np.int32
subjects = [np.array(subject, dtype=dtype) for subject in subjects]
template = np.array(template, dtype=dtype)

# Convert any sequences to single values
subjects = [subject[0] if isinstance(subject, (list, tuple, np.ndarray)) else subject for subject in subjects]
template = template[0] if isinstance(template, (list, tuple, np.ndarray)) else template

# Perform cross-correlation for all subjects
corr_values = normalisecrosscorrelation(subjects,template)

# Print the normalized cross-correlation values for each subject
for i in range(n_subjects):
    print(f"Subject {i+1} correlation value: {corr_values[i]}")

plt.plot(signal1[1, 0, 0])

# Use the first dimension of signal1 as the template
template = signal1[:, 0, 0]

# Slice each subject array to have the same length as the template
subjects = [signal1[1, 0, 0][:len(template)], signal1[2, 0, 0][:len(template)], signal1[3, 0, 0][:len(template)]]

# Perform cross-correlation for all subjects
corr_values = normalisecrosscorrelation(subjects, template)

# Print the normalized cross-correlation values for each subject
for i in range(len(corr_values)):
    print(f"Subject {i+1} correlation value: {corr_values[i]}")

import numpy as np
import matplotlib.pyplot as plt

# Define the number of subjects
num_subjects = 5

# Define the function to calculate the normalized cross-correlation
def normalisecrosscorrelation(x, y):
    x = x - np.mean(x)
    y = y - np.mean(y)
    corr = np.correlate(x, y, mode='full')
    return corr / (np.std(x) * np.std(y))

# Generate random data for demonstration purposes
data = np.random.randn(num_subjects, 1000)

# Loop over the subjects and calculate the cross-correlation
for i in range(num_subjects):
    for j in range(num_subjects):
        if i != j:
            corr = normalisecrosscorrelation(data[i], data[j])
            plt.plot(corr)

plt.show()

import pandas as pd
import scipy.io
import numpy as np
from scipy.signal import butter, lfilter
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the data
mat_file = scipy.io.loadmat('/content/Biosec2_raw_data (2).mat')
signal1 = mat_file['Data_cell']

# Preprocess the data
s1 = signal1[1,0,0]
s1 = np.reshape(s1, 9240)
s1 = s1 - np.mean(s1)
plt.figure(figsize=(12, 6))
plt.plot(signal1[1,0,0],'-b')
for i in range(100):
    s = signal1[i, 0, 0]
    slen = len(s[0])
    s = np.reshape(s, slen)
    s = s - np.mean(s)
    signal1[i, 0, 0] = s
    
    s = s / np.max(np.abs(s))
plt.figure(figsize=(12, 6))    
plt.plot(signal1[1,0,0],'-r')
# Apply a Butterworth bandpass filter
low_cutoff_freq = 0.5 # Cutoff frequency for low-pass filter in Hz
high_cutoff_freq = 10 # Cutoff frequency for high-pass filter in Hz
sampling_rate = 100 # Sampling rate is the number of samples per second
nyquist_freq = 0.5 * sampling_rate
low_normal_cutoff = low_cutoff_freq / nyquist_freq
high_normal_cutoff = high_cutoff_freq / nyquist_freq

# Generate filter coefficients
b, a = butter(4, [low_normal_cutoff, high_normal_cutoff], btype='band')

# Apply the filter to the signal
filtered_signal = lfilter(b, a, s)
segment_length = 100 # Length of each segment
segmented_signal = [] # List to hold the segmented signal

for i in range(0, len(filtered_signal), segment_length):
    # Find peaks in the current segment
    segment_peaks, _ = find_peaks(filtered_signal[i:i+segment_length])
    
    # Add the segment to the segmented signal list
    segmented_signal.append(filtered_signal[i:i+segment_length][segment_peaks])

peaks, _ = find_peaks(filtered_signal1, distance= 20)
peaks = peaks[3:]
s = filtered_signal[peaks[0]:]

dips, _ = find_peaks(-filtered_signal1, distance= 25)
systolic_peaks, _ = find_peaks(filtered_signal1, distance=20 )

if len(dips) > 0 and len(systolic_peaks) > 0:
    diastolic_peaks, _ = find_peaks(-filtered_signal, distance=15)
    systolic_foots, _ = find_peaks(-filtered_signal, distance= 10)
    diastolic_foots, _ = find_peaks(filtered_signal, distance= 15)
    s = filtered_signal[dips[4]:]

    features = []
    for i in range(len(dips)-9):
      if dips[i] - dips[i-1] >= 15 and dips[i+1] - dips[i] >= 15:
        systolic_peak = np.max(filtered_signal[systolic_peaks[i]:dips[i]])
        diastolic_peak = np.min(filtered_signal[diastolic_peaks[i]:systolic_peaks[i+1]])
        feature = [systolic_peak, diastolic_peak]
        features.append(feature)


    # Prepare the data for classification
    X = np.array(features)
    y = np.array([1 if i < 50 else 0 for i in range(100)])

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

    # Normalize the features
    scaler = StandardScaler()

#SVM
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train the SVM classifier
clf = SVC(kernel='rbf', C=1, gamma='scale')
clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)


# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# Make predictions on the test data
y_pred = clf.predict(X_test)


# Get the indices of the correctly and incorrectly classified samples
correct_idx = np.where(y_pred == y_test)[0]
incorrect_idx = np.where(y_pred != y_test)[0]

# Calculate the number of correctly and incorrectly classified samples
n_correct = len(correct_idx)
n_incorrect = len(incorrect_idx)

# Calculate the accuracy, precision, recall, and F1-score
accuracy = n_correct / len(y_test)
precision = n_correct / (n_correct + n_incorrect)
recall = n_correct / np.sum(y_test)
f1_score = 2 * precision * recall / (precision + recall)

# Print the results
print("Number of correctly classified samples: {}".format(n_correct))
print("Number of incorrectly classified samples: {}".format(n_incorrect))
print("Accuracy: {:.2f}%".format(accuracy*100))
print("Precision: {:.2f}%".format(precision*100))
print("Recall: {:.2f}%".format(recall*100))
print("F1-score: {:.2f}%".format(f1_score*100))

# Plot the decision boundary of the SVM classifier
plt.figure(figsize=(8, 6))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='coolwarm')
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create a meshgrid to plot the decision boundary
xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 500),
                     np.linspace(ylim[0], ylim[1], 500))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the margins
ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], alpha=0.5)
ax.set_xlabel('Systolic Peak')
ax.set_ylabel('Diastolic Peak')
ax.set_title('SVM Decision Boundary')

# Show the plot
plt.show()

for idx in incorrect_idx:
    print("Misclassified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))

for idx in correct_idx:
    print("Correctly Classified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))


# Visualize the distribution of the features for each class
plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 0], alpha=0.5, label='Class 1', bins=20)
plt.hist(X_train[y_train == 0, 0], alpha=0.5, label='Class 0', bins=20)
plt.xlabel('Systolic Peak')
plt.ylabel('Frequency')
plt.title('Distribution of Systolic Peak by Class')
plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 1], alpha=0.5, label='Class 1', bins=20)
plt.hist

# Create a meshgrid to plot the decision boundary
xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 500),
                     np.linspace(ylim[0], ylim[1], 500))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the margins
ax.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], alpha=0.5)
ax.set_xlabel('Systolic Peak')
ax.set_ylabel('Diastolic Peak')
ax.set_title('SVM Decision Boundary')

# Show the plot
plt.show()

for idx in incorrect_idx:
    print("Misclassified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))

for idx in correct_idx:
    print("Correctly Classified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))


# Visualize the distribution of the features for each class
plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 0], alpha=0.5, label='Class 1', bins=20)
plt.hist(X_train[y_train == 0, 0], alpha=0.5, label='Class 0', bins=20)
plt.xlabel('Systolic Peak')
plt.ylabel('Frequency')
plt.title('Distribution of Systolic Peak by Class')
plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 1], alpha=0.5, label='Class 1', bins=20)
plt.hist

for idx in incorrect_idx:
    print("Misclassified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))

for idx in correct_idx:
    print("Correctly Classified Sample: {}".format(X_test[idx]))
    print("True Label: {} - Predicted Label: {}%\n".format(y_test[idx], y_pred[idx]))


# Visualize the distribution of the features for each class
plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 0], alpha=0.5, label='Class 1', bins=20)
plt.hist(X_train[y_train == 0, 0], alpha=0.5, label='Class 0', bins=20)
plt.xlabel('Systolic Peak')
plt.ylabel('Frequency')
plt.title('Distribution of Systolic Peak by Class')
plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
plt.hist(X_train[y_train == 1, 1], alpha=0.5, label='Class 1', bins=20)
plt.hist

# RANDOM FOREST
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

#KNN
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Train the KNN classifier
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=10)
clf.fit(X_train, y_train)


# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Train the classifier
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier
clf.fit(X_train, y_train)


# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=42)
clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# Train the classifier
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier
clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)


# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

from sklearn.linear_model import LogisticRegression
clf = LogisticRegression
clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

from sklearn.ensemble import GradientBoostingClassifier
clf = GradientBoostingClassifier
clf.fit(X_train, y_train)

# Predict the test data
y_pred = clf.predict(X_test)

# Predict the test data
y_pred = clf.predict(X_test)

# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)



import pandas as pd

# Convert the features list to a pandas DataFrame
df = pd.DataFrame(features, columns=['systolic_peak', 'diastolic_peak'])

# Compute descriptive statistics
print(df.describe())

# Compute correlation matrix
print(df.corr())

# Perform t-test between the systolic_peak and diastolic_peak features
from scipy.stats import ttest_ind
t_stat, p_val = ttest_ind(df['systolic_peak'], df['diastolic_peak'])
print("t-statistic: {:.3f}, p-value: {:.3f}".format(t_stat, p_val))

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
#plt.xlim(10,100)
plt.plot(filtered_signal1)
plt.plot(peaks, filtered_signal[peaks], "x")
plt.plot(dips, filtered_signal[dips], "o")
plt.title("Filtered signal with detected peaks and dips")
plt.xlabel("Sample number")
plt.ylabel("Amplitude")
plt.show()

import matplotlib.pyplot as plt

# Define a range of C values to test
C_range = [0.1, 1, 10, 100, 1000]

# Store the accuracy scores for each C value
accuracy_scores = []

# Iterate over the range of C values
for C_value in C_range:
    # Train the SVM classifier
    clf = SVC(kernel='rbf', C=C_value, gamma='scale')
    clf.fit(X_train, y_train)

    # Predict the test data
    y_pred = clf.predict(X_test)

    # Evaluate the performance of the classifier
    accuracy = accuracy_score(y_test, y_pred)
    
    # Store the accuracy score
    accuracy_scores.append(accuracy)

# Plot the accuracy scores vs C values
plt.plot(C_range, accuracy_scores, '-o')
plt.xscale('log')
plt.xlabel('C')
plt.ylabel('Accuracy')
plt.title('Accuracy vs C')
plt.show()

signals = [signal1[i] for i in range(1, len(signal1))]
import numpy as np

def normalized_cross_correlation(signal1, signal2):
    """
    Compute the normalized cross-correlation between two signals.
    """
    corr = np.correlate(signal1, signal2, mode='same')
    norm_corr = corr / (np.linalg.norm(signal1) * np.linalg.norm(signal2))
    return norm_corr

# Extract signal1[0] and all other signals in signals
template = signals[2]
subjects = signals[1:]

# Perform normalized cross-correlation between the template and all subjects
corr_values = []
for subject in subjects:
    corr = normalized_cross_correlation(template, subject)
    corr_values.append(corr)

# Print the normalized cross-correlation values for each subject
for i, corr in enumerate(corr_values):
    print(f"Signal {i+2} correlation value: {corr}")

subjects = [filtered_signal1[1],filtered_signal1[2],filtered_signal1[3],filtered_signal1[4],filtered_signal1[5],filtered_signal1[6], filtered_signal1[7],filtered_signal1[8],filtered_signal1[9],filtered_signal1[10],filtered_signal1[11],filtered_signal1[12],filtered_signal1[13],filtered_signal1[14],filtered_signal1[15],filtered_signal1[16],filtered_signal1[17],filtered_signal1[18],filtered_signal1[19],filtered_signal1[20],filtered_signal1[21],filtered_signal1[22],filtered_signal1[23],filtered_signal1[24],filtered_signal1[25],filtered_signal1[26],filtered_signal1[27],filtered_signal1[28],filtered_signal1[29],filtered_signal1[30],filtered_signal1[31],filtered_signal1[32],filtered_signal1[33],filtered_signal1[34],filtered_signal1[35],filtered_signal1[36],filtered_signal1[37],filtered_signal1[38],filtered_signal1[39],filtered_signal1[40],filtered_signal1[41],filtered_signal1[42],filtered_signal1[43],filtered_signal1[44],filtered_signal1[45],filtered_signal1[46],filtered_signal1[47],filtered_signal1[48],filtered_signal1[49],filtered_signal1[50],filtered_signal1[51],filtered_signal1[52],filtered_signal1[53],filtered_signal1[54],filtered_signal1[55],filtered_signal1[56],filtered_signal1[57],filtered_signal1[58],filtered_signal1[59],filtered_signal1[60],filtered_signal1[61],filtered_signal1[62],filtered_signal1[63],filtered_signal1[64],filtered_signal1[65],filtered_signal1[66],filtered_signal1[67],filtered_signal1[68],filtered_signal1[69],filtered_signal1[70],filtered_signal1[71],filtered_signal1[72],filtered_signal1[73],filtered_signal1[74],filtered_signal1[75],filtered_signal1[76],filtered_signal1[77],filtered_signal1[78],filtered_signal1[79],filtered_signal1[80],filtered_signal1[81],filtered_signal1[82],filtered_signal1[83],filtered_signal1[84],filtered_signal1[85],filtered_signal1[86],filtered_signal1[87],filtered_signal1[88],filtered_signal1[89],filtered_signal1[90],filtered_signal1[91],filtered_signal1[92],filtered_signal1[93],filtered_signal1[94],filtered_signal1[95],filtered_signal1[96],filtered_signal1[97],filtered_signal1[98],filtered_signal1[99],filtered_signal1[100]]

filtered_signal1 = [filtered_signal1[1],filtered_signal1[2],filtered_signal1[3],filtered_signal1[4],filtered_signal1[5],filtered_signal1[6], filtered_signal1[7],filtered_signal1[8],filtered_signal1[9],filtered_signal1[10],filtered_signal1[11],filtered_signal1[12],filtered_signal1[13],filtered_signal1[14],filtered_signal1[15],filtered_signal1[16],filtered_signal1[17],filtered_signal1[18],filtered_signal1[19],filtered_signal1[20],filtered_signal1[21],filtered_signal1[22],filtered_signal1[23],filtered_signal1[24],filtered_signal1[25],filtered_signal1[26],filtered_signal1[27],filtered_signal1[28],filtered_signal1[29],filtered_signal1[30],filtered_signal1[31],filtered_signal1[32],filtered_signal1[33],filtered_signal1[34],filtered_signal1[35],filtered_signal1[36],filtered_signal1[37],filtered_signal1[38],filtered_signal1[39],filtered_signal1[40],filtered_signal1[41],filtered_signal1[42],filtered_signal1[43],filtered_signal1[44],filtered_signal1[45],filtered_signal1[46],filtered_signal1[47],filtered_signal1[48],filtered_signal1[49],filtered_signal1[50],filtered_signal1[51],filtered_signal1[52],filtered_signal1[53],filtered_signal1[54],filtered_signal1[55],filtered_signal1[56],filtered_signal1[57],filtered_signal1[58],filtered_signal1[59],filtered_signal1[60],filtered_signal1[61],filtered_signal1[62],filtered_signal1[63],filtered_signal1[64],filtered_signal1[65],filtered_signal1[66],filtered_signal1[67],filtered_signal1[68],filtered_signal1[69],filtered_signal1[70],filtered_signal1[71],filtered_signal1[72],filtered_signal1[73],filtered_signal1[74],filtered_signal1[75],filtered_signal1[76],filtered_signal1[77],filtered_signal1[78],filtered_signal1[79],filtered_signal1[80],filtered_signal1[81],filtered_signal1[82],filtered_signal1[83],filtered_signal1[84],filtered_signal1[85],filtered_signal1[86],filtered_signal1[87],filtered_signal1[88],filtered_signal1[89],filtered_signal1[90],filtered_signal1[91],filtered_signal1[92],filtered_signal1[93],filtered_signal1[94],filtered_signal1[95],filtered_signal1[96],filtered_signal1[97],filtered_signal1[98],filtered_signal1[99],filtered_signal1[100]]
cross_corr = []
for i in range(100):
        if i == 0:
            continue
        signal2 = filtered_signal1[i]
        
        corr = correlate(filtered_signal1, filtered_signal2, mode='same')
        cross_corr.append(corr)

import pandas as pd
import scipy.io
import numpy as np
from scipy.signal import butter, lfilter, correlate
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the data
mat_file = scipy.io.loadmat('/content/Biosec2_raw_data (2).mat')
signals = mat_file['Data_cell']

# Preprocess the data
signal1 = signals[1,0,0]
signal1 = np.reshape(signal1, 9240)
signal1 = signal1 - np.mean(signal1)

for i in range(100):
    s = signals[i, 0, 0]
    slen = len(s[0])
    s = np.reshape(s, slen)
    s = s - np.mean(s)
    signals[i, 0, 0] = s
    s = s / np.max(np.abs(s))

# Filter the signal
cutoff_freq = 0.5
sampling_rate = 100
nyquist_freq = 0.5 * sampling_rate
normal_cutoff = cutoff_freq / nyquist_freq
b, a = butter(1, normal_cutoff, btype='low', analog=False)
filtered_signal1 = lfilter(b, a, signal1)

# Find the peaks and foots of the signal
peaks, _ = find_peaks(filtered_signal1, distance= 20)
peaks = peaks[3:]
s = filtered_signal1[peaks[0]:]

dips, _ = find_peaks(-filtered_signal1, distance= 25)
systolic_peaks, _ = find_peaks(filtered_signal1, distance=20 )

if len(dips) > 0 and len(systolic_peaks) > 0:
    diastolic_peaks, _ = find_peaks(-filtered_signal1, distance=15)
    systolic_foots, _ = find_peaks(-filtered_signal1, distance= 10)
    diastolic_foots, _ = find_peaks(filtered_signal1, distance= 15)
    s = filtered_signal1[dips[4]:]

    # Compute the cross-correlation between signal1 and all other signals in the dataset
    cross_corr = []
    for i in range(100):
        if i == 0:
            continue
        signal2 = subjects[i]
        signal2 = np.reshape(signal2, 9000)
        signal2 = signal2 - np.mean(signal2)
        signal2 = signal2 / np.max(np.abs(signal2))
        filtered_signal2 = lfilter(b, a, signal2)
        corr = correlate(filtered_signal1, filtered_signal2, mode='same')
        cross_corr.append(corr)

    # Prepare the data for classification
    X = np.array(cross_corr).T
    y = np.array([1 if i < 50 else 0 for i in range(100)])

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train the SVM classifier
    clf = SVC(kernel='rbf', C=1, gamma='scale')